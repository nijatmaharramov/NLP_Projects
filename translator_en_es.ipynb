{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nijatmaharramov/NLP_Projects/blob/main/translator_en_es.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this project we will create translator from English To Spanish by using different methods(RNN, Transformer model etc.)."
      ],
      "metadata": {
        "id": "h340jvQ4F8N6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# An encoder- decoder Neural Machine Translation"
      ],
      "metadata": {
        "id": "VGi5fiiK3UIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# First we download the data\n",
        "from pathlib import Path\n",
        "import tensorflow as tf\n",
        "\n",
        "url = \"https://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\"\n",
        "path = tf.keras.utils.get_file(\"spa-eng.zip\", origin=url, cache_dir=\"datasets\", extract=True)\n",
        "\n",
        "# Final corrected path\n",
        "spa_txt_path = Path(path).parent / \"spa-eng_extracted\" / \"spa-eng\" / \"spa.txt\"\n",
        "\n",
        "# Read the file\n",
        "text = spa_txt_path.read_text(encoding='utf-8')\n",
        "print(text[:500])  # Print first 500 characters as a quick check"
      ],
      "metadata": {
        "id": "MFlYUT8O3T9k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "text = text.replace(\"¡\", \"\").replace(\"¿\",\"\")\n",
        "pairs = [line.split(\"\\t\") for line in text.splitlines()]\n",
        "np.random.seed(42) # extra code - ensures reproducibility on CPU\n",
        "np.random.shuffle(pairs)\n",
        "sentences_en, sentences_es = zip(*pairs) # seperates the pairs into to lists"
      ],
      "metadata": {
        "id": "MaEBUAFtrr8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(3):\n",
        "    print(sentences_en[i], '=>', sentences_es[i])"
      ],
      "metadata": {
        "id": "8XfhdVGB4hTS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 1000\n",
        "max_length = 50\n",
        "\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length\n",
        ")\n",
        "\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length\n",
        ")\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])"
      ],
      "metadata": {
        "id": "urtS0bk65BoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_en.get_vocabulary()[:10] # most used words in English vocabulary"
      ],
      "metadata": {
        "id": "fBscZy8Z7Slc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_vec_layer_es.get_vocabulary()[:10]"
      ],
      "metadata": {
        "id": "NeEvtyBS7Sq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train  = tf.constant(sentences_en[:100_000])\n",
        "X_valid = tf.constant(sentences_en[100_000:])\n",
        "X_train_dec = tf.constant([f'startofseq {s}' for s in sentences_es[:100_000]])\n",
        "X_valid_dec = tf.constant([f'startofseq {s}' for s in sentences_es[100_000:]])\n",
        "Y_train = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[:100_000]])\n",
        "Y_valid = text_vec_layer_es([f'{s} endofseq' for s in sentences_es[100_000:]])"
      ],
      "metadata": {
        "id": "g7qwqz5r7b-o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "nSUaSL_48knw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size=128\n",
        "\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_inputs_ids = text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero=True)\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero=True)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs_ids)"
      ],
      "metadata": {
        "id": "CeyI3RYV94Re"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = tf.keras.layers.LSTM(512, return_state = True)\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)"
      ],
      "metadata": {
        "id": "iwaOIGEI-orx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state = encoder_state)"
      ],
      "metadata": {
        "id": "_8EFzUXK_iN-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_layer = tf.keras.layers.Dense(vocab_size, activation = 'softmax')\n",
        "Y_proba = output_layer(decoder_outputs)"
      ],
      "metadata": {
        "id": "dAARM5u-Ai0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Model(inputs = [encoder_inputs, decoder_inputs],\n",
        "                       outputs = [Y_proba])\n",
        "\n",
        "model.compile(loss = 'sparse_categorical_crossentropy',\n",
        "              optimizer = 'nadam',\n",
        "              metrics = ['accuracy'])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs = 3,\n",
        "          validation_data = ((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "7zGjOD8GBXx1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def translate(sentence_en):\n",
        "    translation = ''\n",
        "    X = tf.constant([sentence_en])\n",
        "\n",
        "    for word_idx in range(max_length):\n",
        "        X_dec = tf.constant(['startofseq' + translation])\n",
        "        y_proba = model.predict((X, X_dec))[0, word_idx]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "        predicted_word = text_vec_layer_es.get_vocabulary()[predicted_word_id]\n",
        "        if predicted_word == 'endofseq':\n",
        "            break\n",
        "        translation += ' ' + predicted_word\n",
        "    return translation.strip()\n"
      ],
      "metadata": {
        "id": "dEt7YSfjB5ZP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "translate('i like soccer')"
      ],
      "metadata": {
        "id": "fEx1gJVRJvGO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bidirectional RNNs"
      ],
      "metadata": {
        "id": "Z3rKx41iKv89"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_state = True)\n",
        ")"
      ],
      "metadata": {
        "id": "jl2AEO0TLqoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ConcatenateStates(tf.keras.layers.Layer):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def call(self, encoder_state):\n",
        "        return [tf.concat(encoder_state[::2], axis=-1), # short-term (0 & 2)\n",
        "                tf.concat(encoder_state[1::2], axis=-1)] # long_term (1 & 3)\n",
        "\n",
        "encoder_outputs, *encoder_state = encoder(encoder_embeddings)\n",
        "concat_states = ConcatenateStates()\n",
        "encoder_state = concat_states(encoder_state)"
      ],
      "metadata": {
        "id": "ZfvsKiIXL2sw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences = True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state = encoder_state)\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation = 'softmax')\n",
        "Y_proba = output_layer(decoder_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs = [encoder_inputs, decoder_inputs],\n",
        "                       outputs = [Y_proba])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='nadam',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs = 4,\n",
        "          validation_data = ((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "aCR-0QbGOkMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Beam Search"
      ],
      "metadata": {
        "id": "wzj3mQm5RfQ8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def beam_search(sentence_en, beam_width, verbose=False):\n",
        "    X = tf.constant([sentence_en])  # encoder input\n",
        "    X_dec = tf.constant([\"startofseq\"])  # decoder input\n",
        "    y_proba = model.predict((X, X_dec))[0, 0]  # first token's probas\n",
        "    top_k = tf.math.top_k(y_proba, k=beam_width)\n",
        "    top_translations = [  # list of best (log_proba, translation)\n",
        "        (np.log(word_proba), text_vec_layer_es.get_vocabulary()[word_id])\n",
        "        for word_proba, word_id in zip(top_k.values, top_k.indices)\n",
        "    ]\n",
        "\n",
        "    # extra code – displays the top first words in verbose mode\n",
        "    if verbose:\n",
        "        print(\"Top first words:\", top_translations)\n",
        "\n",
        "    for idx in range(1, max_length):\n",
        "        candidates = []\n",
        "        for log_proba, translation in top_translations:\n",
        "            if translation.endswith(\"endofseq\"):\n",
        "                candidates.append((log_proba, translation))\n",
        "                continue  # translation is finished, so don't try to extend it\n",
        "            X = tf.constant([sentence_en])  # encoder input\n",
        "            X_dec = tf.constant([\"startofseq \" + translation])  # decoder input\n",
        "            y_proba = model.predict((X, X_dec))[0, idx]  # last token's proba\n",
        "            for word_id, word_proba in enumerate(y_proba):\n",
        "                word = text_vec_layer_es.get_vocabulary()[word_id]\n",
        "                candidates.append((log_proba + np.log(word_proba),\n",
        "                                   f\"{translation} {word}\"))\n",
        "        top_translations = sorted(candidates, reverse=True)[:beam_width]\n",
        "\n",
        "        # extra code – displays the top translation so far in verbose mode\n",
        "        if verbose:\n",
        "            print(\"Top translations so far:\", top_translations)\n",
        "\n",
        "        if all([tr.endswith(\"endofseq\") for _, tr in top_translations]):\n",
        "            return top_translations[0][1].replace(\"endofseq\", \"\").strip()"
      ],
      "metadata": {
        "id": "We92CnWXP5se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_en = 'I like soccer and going to the beach'\n",
        "translate(sentence_en)"
      ],
      "metadata": {
        "id": "XaBmAi5JRxe1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "beam_search(sentence_en, beam_width = 3, verbose=True)"
      ],
      "metadata": {
        "id": "KEiMtKUqRVz-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Attention Mechanisms"
      ],
      "metadata": {
        "id": "33rAwbfoRvbE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.random.set_seed(42)\n",
        "encoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)\n",
        "decoder_inputs = tf.keras.layers.Input(shape=[], dtype=tf.string)"
      ],
      "metadata": {
        "id": "4zMiC864Scb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embed_size = 128\n",
        "\n",
        "encoder_input_ids = text_vec_layer_en(encoder_inputs)\n",
        "decoder_input_ids = text_vec_layer_es(decoder_inputs)\n",
        "\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size)\n",
        "\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_input_ids)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_input_ids)"
      ],
      "metadata": {
        "id": "uzthU6OcScht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoding = tf.keras.layers.Bidirectional(\n",
        "    tf.keras.layers.LSTM(256, return_sequences = True, return_state = True)\n",
        "                                        )\n",
        "\n",
        "encoder_outputs, *encoder_state = encoding(encoder_embeddings)\n",
        "encoder_state = concat_states(encoder_state)"
      ],
      "metadata": {
        "id": "V8ED6JZfVadl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = tf.keras.layers.LSTM(512, return_sequences=True)\n",
        "decoder_outputs = decoder(decoder_embeddings, initial_state=encoder_state)"
      ],
      "metadata": {
        "id": "uZKUpRviWHW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = tf.keras.layers.Attention()\n",
        "attention_outputs = attention_layer([decoder_outputs, encoder_outputs])\n",
        "\n",
        "output_layer = tf.keras.layers.Dense(vocab_size, activation='softmax')\n",
        "Y_proba = output_layer(attention_outputs)\n",
        "\n",
        "model = tf.keras.Model(inputs=[encoder_inputs, decoder_inputs],\n",
        "                       outputs=[Y_proba])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='nadam', metrics=['accuracy'])\n",
        "\n",
        "model.fit((X_train, X_train_dec), Y_train, epochs=3,\n",
        "          validation_data=((X_valid, X_valid_dec), Y_valid))"
      ],
      "metadata": {
        "id": "QiKMTOBjXKlo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_en = 'I like soccer and going to the beach'\n",
        "translate(sentence_en)"
      ],
      "metadata": {
        "id": "ArtQqjytX_rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Transformer Model"
      ],
      "metadata": {
        "id": "Pd2LWXXBcMPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "vocab_size = 10000\n",
        "max_length = 50\n",
        "embed_size = 128\n",
        "num_heads = 5\n",
        "ff_dim = 512\n",
        "\n",
        "#Input layers\n",
        "encoder_inputs = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"encoder_inputs\")\n",
        "decoder_inputs = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"decoder_inputs\")\n",
        "\n",
        "\n",
        "\n",
        "#Embedding layer\n",
        "encoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero = True)\n",
        "\n",
        "decoder_embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                                    output_dim = embed_size,\n",
        "                                                    mask_zero = True)\n",
        "\n",
        "encoder_embeddings = encoder_embedding_layer(encoder_inputs)\n",
        "decoder_embeddings = decoder_embedding_layer(decoder_inputs)\n",
        "\n",
        "\n",
        "#Positional embeddings\n",
        "pos_embedding_layer = tf.keras.layers.Embedding(max_length, embed_size)\n",
        "positions_encoder = tf.keras.layers.Lambda(lambda x: tf.range(start = 0, limit = tf.shape(x)[1], delta = 1))(encoder_inputs)\n",
        "positions_decoder = tf.keras.layers.Lambda(lambda x: tf.range(start = 0, limit = tf.shape(x)[1], delta = 1))(decoder_inputs)\n",
        "pos_embed_enc = pos_embedding_layer(positions_encoder)\n",
        "pos_embed_dec = pos_embedding_layer(positions_decoder)\n",
        "\n",
        "#Add tokens and positional embeddings\n",
        "encoder_embed = encoder_embeddings + pos_embed_enc\n",
        "decoder_embed = decoder_embeddings + pos_embed_dec\n",
        "\n",
        "\n",
        "#Encoder self-attention\n",
        "encoder_attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_size)(encoder_embed, encoder_embed)\n",
        "encoder_attention = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(encoder_embed + encoder_attention)\n",
        "\n",
        "#Encoder Feed-forward\n",
        "encoder_ff = tf.keras.layers.Dense(ff_dim, activation = 'relu')(encoder_attention)\n",
        "encoder_ff = tf.keras.layers.Dense(embed_size)(encoder_ff)\n",
        "encoder_outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(encoder_attention + encoder_ff)\n",
        "\n",
        "\n",
        "#Decoder self-attention\n",
        "causal_mask = tf.keras.layers.Lambda(\n",
        "    lambda x: tf.linalg.band_part(tf.ones((tf.shape(x)[1], tf.shape(x)[1])), -1, 0)\n",
        ")(decoder_inputs)\n",
        "decoder_attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim=embed_size)(decoder_embed, decoder_embed, attention_mask = causal_mask)\n",
        "decoder_attention = tf.keras.layers.LayerNormalization(epsilon= 1e-6)(decoder_embed + decoder_attention)\n",
        "\n",
        "#Encoder-Decoder Cross Attention\n",
        "cross_attention = tf.keras.layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_size)(decoder_attention, encoder_outputs, encoder_outputs)\n",
        "decoder_cross = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(decoder_attention + cross_attention)\n",
        "\n",
        "\n",
        "\n",
        "#Decoder feed-forward\n",
        "decoder_ff = tf.keras.layers.Dense(ff_dim, activation = 'relu')(decoder_cross)\n",
        "decoder_ff = tf.keras.layers.Dense(embed_size)(decoder_ff)\n",
        "decoder_outputs = tf.keras.layers.LayerNormalization(epsilon = 1e-6)(decoder_cross + decoder_ff)\n",
        "\n",
        "\n",
        "#Final output layer\n",
        "output_logits = tf.keras.layers.Dense(vocab_size, activation = 'softmax')(decoder_outputs)\n",
        "\n",
        "#Model\n",
        "transformer = tf.keras.Model([encoder_inputs, decoder_inputs], output_logits)"
      ],
      "metadata": {
        "id": "4oZ66-p_kTLo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer.compile(\n",
        "    loss='sparse_categorical_crossentropy',\n",
        "    optimizer='nadam',\n",
        "    metrics=['accuracy']\n",
        ")"
      ],
      "metadata": {
        "id": "05PPBgGGkTQF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = 10000\n",
        "max_length = 50\n",
        "\n",
        "text_vec_layer_en = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length,\n",
        "    pad_to_max_tokens=True\n",
        ")\n",
        "\n",
        "text_vec_layer_es = tf.keras.layers.TextVectorization(\n",
        "    vocab_size, output_sequence_length = max_length,\n",
        "    pad_to_max_tokens=True\n",
        ")\n",
        "\n",
        "text_vec_layer_en.adapt(sentences_en)\n",
        "text_vec_layer_es.adapt([f'startofseq {s} endofseq' for s in sentences_es])"
      ],
      "metadata": {
        "id": "kdGCyiwXkNOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_en(X_train).numpy(), padding='post', maxlen=max_length\n",
        ")\n",
        "\n",
        "X_train_dec_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_es(X_train_dec).numpy(), padding='post', maxlen=max_length\n",
        ")\n",
        "\n",
        "X_valid_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_en(X_valid).numpy(), padding='post', maxlen=max_length\n",
        ")\n",
        "\n",
        "X_valid_dec_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "    text_vec_layer_en(X_valid_dec).numpy(), padding='post', maxlen=max_length\n",
        ")\n",
        "\n",
        "\n",
        "X_train_padded = tf.constant(X_train_padded)\n",
        "X_train_dec_padded = tf.constant(X_train_dec_padded)\n",
        "X_valid_padded = tf.constant(X_valid_padded)\n",
        "X_valid_dec_padded = tf.constant(X_valid_dec_padded)\n",
        "\n",
        "transformer.fit(\n",
        "    (X_train_padded, X_train_dec_padded),\n",
        "    Y_train, epochs=3,\n",
        "    validation_data = ((X_valid_padded, X_valid_dec_padded), Y_valid,))"
      ],
      "metadata": {
        "id": "JlIqysuc0W18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def translate(sentence_en):\n",
        "    # Tokenize and pad encoder input\n",
        "    X = text_vec_layer_en(tf.constant([sentence_en]))\n",
        "    X = tf.keras.preprocessing.sequence.pad_sequences(X.numpy(), padding=\"post\", maxlen=max_length)\n",
        "\n",
        "    # Start token\n",
        "    start_token = text_vec_layer_es([ 'startofseq'])[0][0]\n",
        "    end_token = text_vec_layer_es(['endofseq'])[0][0]\n",
        "\n",
        "    # Decoder input initialized with just the start token\n",
        "    decoder_input = [start_token]\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        decoder_input_padded = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "            [decoder_input], maxlen=max_length, padding=\"post\"\n",
        "        )\n",
        "\n",
        "        y_proba = transformer.predict((X, decoder_input_padded), verbose=0)[0, len(decoder_input)-1]\n",
        "        predicted_word_id = np.argmax(y_proba)\n",
        "\n",
        "        if predicted_word_id == end_token:\n",
        "            break\n",
        "\n",
        "        decoder_input.append(predicted_word_id)\n",
        "\n",
        "    # Map tokens back to words\n",
        "    vocab = text_vec_layer_es.get_vocabulary()\n",
        "    translated_words = [vocab[token] for token in decoder_input[1:]]  # skip start token\n",
        "\n",
        "    return ' '.join(translated_words)"
      ],
      "metadata": {
        "id": "GLrY0MML3fO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentence_en = 'I like soccer and going to the beach'\n",
        "translate(sentence_en)"
      ],
      "metadata": {
        "id": "cHoSkpE9SF4N"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}