# NLP_Projects
# ğŸ‡¬ğŸ‡§â†’ğŸ‡ªğŸ‡¸ Transformer-Based English to Spanish Translator

This project implements a custom **Transformer-based Neural Machine Translation (NMT)** system to translate English sentences into Spanish.

Built entirely using TensorFlow and Keras, the model mimics the "Attention Is All You Need" architecture with custom embeddings, multi-head attention, masked decoding, and greedy inference.

---

## ğŸ”§ Features

- âœ… Custom Transformer encoder-decoder
- âœ… Positional encoding with learnable embeddings
- âœ… Multi-head self-attention & cross-attention
- âœ… Causal mask for autoregressive decoding
- âœ… Tokenization with `TextVectorization` layers
- âœ… Greedy decoding at inference time

---

## ğŸ“ Project Structure
The project is currently contained within a single Colab notebook, which includes all the necessary steps for building and training the English-to-Spanish translation model. The notebook is organized as follows:
â”œâ”€â”€ translator_en_es.ipynb 
â””â”€â”€ README.md # Project documentation


